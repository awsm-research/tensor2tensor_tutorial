import sys, os
from collections import Counter

import tensorflow as tf
from tensor2tensor.utils import trainer_lib
from tensor2tensor.data_generators import problem, text_problems
from tensor2tensor.utils import registry
from tensor2tensor.utils.trainer_lib import create_run_config, create_experiment, create_hparams
from tensor2tensor.models import transformer
from tensor2tensor import models, problems

from torchtext.vocab import Vocab

tf.compat.v1.enable_eager_execution()

# Setup and create directories.


# path that stores directory namely 'train' 'eval' 'test'
# data_path = '<dataset path>'

DATA_DIR = os.path.join('../generated_data/<directory to store data generated by tensor2tensor>')
OUTPUT_DIR = os.path.join('../output/<directory to store output>')
MODEL_DIR = os.path.join('../models/<directory to store model>')

# Create them.
tf.io.gfile.makedirs(DATA_DIR)
tf.io.gfile.makedirs(OUTPUT_DIR)
tf.io.gfile.makedirs(MODEL_DIR)

# class name must be exactly the same as python file that stores problem configuration
@registry.register_problem
class SampleProblem(text_problems.Text2TextProblem):
    @property
    def vocab_type(self):
    # We can use different types of vocabularies, `VocabType.CHARACTER`,
    # `VocabType.SUBWORD` and `VocabType.TOKEN`.
    #
    # SUBWORD and CHARACTER are fully invertible -- but SUBWORD provides a good
    # tradeoff between CHARACTER and TOKEN.
        return text_problems.VocabType.TOKEN

    @property
    def oov_token(self):
        """Out of vocabulary token. Only for VocabType.TOKEN."""
        return '<UNK>'

    @property
    def is_generate_per_split(self):
        # If we have pre-existing data splits for (train, eval, test) then we set
        # this to True, which will have generate_samples be called for each of the
        # dataset_splits.
        #
        # If we do not have pre-existing data splits, we set this to False, which
        # will have generate_samples be called just once and the Problem will
        # automatically partition the data into dataset_splits.
        return True

    def generate_samples(self, data_dir, tmp_dir, dataset_split):
        train = dataset_split == problem.DatasetSplit.TRAIN
        if dataset_split == problem.DatasetSplit.TRAIN:
            file_path = '<path to directory that stores train data>'
        elif dataset_split == problem.DatasetSplit.EVAL:
            file_path = '<path to directory that stores validation data>'
        elif dataset_split == problem.DatasetSplit.TEST:
            file_path = '<path to directory that stores test data>'

        # here input and ground truth are text files
        # where one line of input is string and one line of ground truth is string

        # read input of model here (note: before.txt can be anything)
        with open(os.path.join(file_path, 'before.txt'), 'r') as f:
            input = f.readlines()

        # read ground truth here (note: after.txt can be anything)
        with open(os.path.join(file_path, 'after.txt'), 'r') as f:
            ground_truth = f.readlines()

        # append <EOS> token here
        # don't change this!!

        input_code_processed = [s.replace('\n',' <EOS>').strip() for s in buggy_code]
        ground_truth_code_processed = [s.replace('\n',' <EOS>').strip() for s in fixed_code]

        # leave this as-is
        for i in range(len(input_code_processed)):
            yield{
                "inputs": input_code_processed[i],
                "targets": ground_truth_code_processed[i]
            }

    # a function for generating vocab file
    # use this if vocab_type returns text_problems.VocabType.TOKEN
    def generate_vocab(self, data_dir):
        file_path = '<path to directory that stores train data>'

        # read input of model here (note: before.txt can be anything)
        with open(os.path.join(file_path, 'before.txt'), 'r') as f:
            input = f.readlines()

        # read ground truth here (note: after.txt can be anything)
        with open(os.path.join(file_path, 'after.txt'), 'r') as f:
            ground_truth = f.readlines()

        input_code_processed = [s.replace('\n',' <EOS>').strip() for s in buggy_code]
        ground_truth_code_processed = [s.replace('\n',' <EOS>').strip() for s in fixed_code]

        vocab_counter = Counter()

        for line in input_code_processed:
            vocab_counter.update(line.split())      # split by space so java.util is seen as 1 token

        for line in ground_truth_code_processed:
            vocab_counter.update(line.split())

        vocab = Vocab(vocab_counter)
        vocab_list = list(vocab.freqs.keys())

        # don't change this line!! 
        vocab_list = ["<pad>", "<EOS>"] + vocab_list + ['<UNK>'] 

        vocab_str = '\n'.join(vocab_list)

        # don't change this line!! Otherwise error will occur.
        with open(os.path.join(data_dir,'vocab.'+self.name+'.tokens'),'w') as f:
            f.write(vocab_str)

    @property
    def dataset_splits(self):
        return [{
            "split": problem.DatasetSplit.TRAIN,
            "shards": 100,
        }, {
            "split": problem.DatasetSplit.EVAL,
            "shards": 100,
        }]

my_problem = LearnCodeChange()

my_problem.generate_vocab(DATA_DIR)
my_problem.generate_data(DATA_DIR, TMP_DIR)