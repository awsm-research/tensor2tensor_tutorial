# same as train_model.sh
PROBLEM=learn_code_change
MODEL=transformer
usr_dir=./learn_code_change


# for evaluation
BEAM_SIZE=10
ALPHA=0.6       # The default value of tensor2tensor, you may increase this if input is long


DECODE_FILE='<path to text file that stores ground-truth>' # e.g., dataset/proj1/test/ground-truth.txt

# same as train_model.sh
HPARAMS=transformer_hparams1

MODEL_DIR='<directory to stores trained models>'
DATA_DIR='<directory that stores data generated by tensor2tensor>'
RESULT_DIR='<directory that stores prediction result generated by tensor2tensor>'

mkdir -p $RESULT_DIR

# here $ckpt_eval_step is the number of train step to evaluate model
ckpt_eval_step=12000
CKPT_PATH=$MODEL_DIR/model.ckpt-$ckpt_eval_step

decode_to_file=$RESULT_DIR/'<result file name>'

# set return_beams=True if you want to do beam search
# set max_input_size=<xxx> to limit input size of transformer (reduce this value if you cannot run transformer decoder)
t2t-decoder     --data_dir=$DATA_DIR     --problem=$PROBLEM     --model=$MODEL     --hparams_set=$HPARAMS     --output_dir=$MODEL_DIR     --checkpoint_path=$CKPT_PATH     --decode_hparams="beam_size=1,alpha=$ALPHA,return_beams=True,max_input_size=1200"     --decode_from_file=$DECODE_FILE     --decode_to_file=$decode_to_file     --t2t_usr_dir=$usr_dir
