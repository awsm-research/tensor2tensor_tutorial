PROBLEM=sample_problem              # change this to directory that store python problem configuration file
MODEL=transformer                   # we use Transformer here, so don't change this
eval_steps=100                      # The default value of tensor2tensor
save_checkpoints_steps=1000         # Save model at every 1000 train step, you may change this if you want
schedule=continuous_train_and_eval  # The default value of tensor2tensor
usr_dir=./sample_problem            # same as PROBLEM variable
   
# Get transformer parameter from python problem configuration file (here it is in ./sample_problem/SampleProblem_problem.py)
# The parameter name in the python file is TransformerHparams1
# The name is split by '_' 
# The process is something like TransformerHparams1 -> Transformer_Hparams1 -> transformer_hparams1
# This means the captial letters are used as words splitter (as shown above) 
HPARAMS=transformer_hparams1

MODEL_DIR='<directory to stores trained models>'
DATA_DIR='<directory that stores data generated by tensor2tensor>'

mkdir -p $MODEL_DIR

# keep_checkpoint_max is the number of latest models to keep during training phase
# here tensor2tensor will keep 10 latest models
t2t-trainer --data_dir=$DATA_DIR --problem=$PROBLEM --eval_throttle_seconds=10 --model=$MODEL --hparams_set=$HPARAMS --keep_checkpoint_max=10 --schedule=$schedule --output_dir=$MODEL_DIR --train_steps=$train_steps --eval_steps=$eval_steps --t2t_usr_dir=$usr_dir --random_seed 0
